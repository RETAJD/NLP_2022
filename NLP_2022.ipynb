{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnfZRgOZsltg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Dict\n",
        "import networkx as nx\n",
        "from networkx.algorithms import bipartite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOHZi3FPf6Y7"
      },
      "source": [
        "**PUNKT 1**\n",
        "```\n",
        "Ranking    Wyraz(str).     F[procent] - czestotliwosc.      r x f(słupkami poziomymi)\n",
        "```\n",
        "Manuskrypt wojnicza oraz dla dowolnego języka naturalnego\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I379YgN1Vmn"
      },
      "source": [
        "Wybranie odpowiedniego tekstu do przetwarzania. str: [1- 118]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veDBYFNJ1V9h"
      },
      "outputs": [],
      "source": [
        "with open(\"FSG.txt\", \"r\") as file, open('wynik.txt', 'w') as destination:\n",
        "  for idx, line in enumerate(file):\n",
        "      if \"page\" in line or (\"#\" not in line and \"$\" not in line):\n",
        "        destination.write(line)\n",
        "      if idx == 2679: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTjzYsh8AXkW"
      },
      "source": [
        "Stworzenie listy zawierającej same słowa\n",
        "i wyświetlenie ilości słów"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XmjHVM93jTN"
      },
      "outputs": [],
      "source": [
        "raw_text = []\n",
        "with open(\"wynik.txt\", \"r\") as wynik:\n",
        "  for line in filter(lambda l: \"#\" not in l, wynik):\n",
        "    words: List[str] = line.strip().split(',')\n",
        "    if '-' in words[-1]  or '=' in words[-1]:\n",
        "      words[-1] = words[-1].replace(\"=\",'').replace(\"-\",'')\n",
        "    raw_text.extend(word for word in words if word)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n87k66nTVGv4"
      },
      "source": [
        "Policzenie ilości danego słowa w tekscie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6xQBGB8AgHC"
      },
      "outputs": [],
      "source": [
        "counted: List[Tuple[str, int]] = [(unique_word, raw_text.count(unique_word)) for unique_word in filter(lambda word: word, set(raw_text))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfW7CCNCsUUU"
      },
      "outputs": [],
      "source": [
        "counted.sort(key=lambda x: -x[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWjOCgWHtGci"
      },
      "outputs": [],
      "source": [
        "print(\"rank|word|ratio|chart\")\n",
        "ytics: List[str] = []\n",
        "ratios: List[float] = []\n",
        "for idx, (word, occurences) in enumerate(counted, 1):\n",
        "  ratio: float = occurences/len(raw_text)*100\n",
        "  ytics.append(f'{idx} | {word} | {ratio} | ')\n",
        "  ratios.append(ratio*idx)\n",
        "\n",
        "plt.figure(figsize=(30, 650))\n",
        "plt.barh(ytics, ratios)\n",
        "# plt.xlim([0,5])\n",
        "plt.grid()\n",
        "plt.xlabel(\"r x f\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWg7mVakoVwV"
      },
      "source": [
        "**PUNKT 2**\n",
        "\n",
        "Sąsiedztwo między wyrazami\n",
        "Sąsiedztwo to dwa wyrazy do tylu i dwa wyrazy do przodu.\n",
        "```\n",
        "W porządku alfabetycznym\n",
        "Wyr1.           Wyr1\n",
        "Wyr2.           Wyr2\n",
        "Wyr3.           Wyr3\n",
        "Wyr4.           Wyr4\n",
        "Wyr5.           Wyr5\n",
        "```\n",
        "I robimy strzaleczkami połączenia między wyrazami sąsiadującymi.\n",
        "\n",
        "Powinien się znaleźć wyraz który ma znacznie dużo sąsiadów - to jest rdzeń języka.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-Soe-Ij9pEZ"
      },
      "outputs": [],
      "source": [
        "unique_words: List[str] = [word for word, _ in counted]\n",
        "\n",
        "\n",
        "# dla kazdego wyrazu z listy unique_words musimy znalezc ten wyraz w licie raw_text i do niego dopisac sasiadow\n",
        "nested_structure: Dict[str, Dict[str, int]] = {}\n",
        "plain_structure: Dict[str, int] = {unique_word: 0 for unique_word in unique_words}\n",
        "for idx, word in enumerate(raw_text[:-2]):\n",
        "    for neigh_idx, neigh in enumerate(raw_text[idx:idx+3]):\n",
        "        a, b = (word, neigh) if word<neigh else (neigh, word)\n",
        "        if not a in nested_structure:\n",
        "            nested_structure[a] = {}\n",
        "        if not b in nested_structure[a]:\n",
        "            nested_structure[a][b] = 0\n",
        "        nested_structure[a][b] += 1\n",
        "        plain_structure[a]+=1\n",
        "        plain_structure[b]+=1\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnPr1-3h9pEZ"
      },
      "outputs": [],
      "source": [
        "N: int = 20\n",
        "links_threshold: int = 10\n",
        "\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from([node for node, links_count in plain_structure.items() if links_count > links_threshold], bipartite=0)\n",
        "G.add_nodes_from([node+' ' for node, links_count in plain_structure.items() if links_count > links_threshold], bipartite=1)\n",
        "G.add_edges_from((unique_word, neighbour+\" \") for unique_word, neighbours in nested_structure.items() for neighbour, count in neighbours.items() if count > N)\n",
        "\n",
        "\n",
        "is_bipartite: bool = bipartite.is_bipartite(G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh5FOu439pEa"
      },
      "outputs": [],
      "source": [
        "plt.hist([count for neighbours in nested_structure.values() for count in neighbours.values()], bins=500)\n",
        "plt.xlim([0, 20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOM2CEWi9pEb"
      },
      "outputs": [],
      "source": [
        "print(max(plain_structure, key = lambda key: plain_structure[key]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMq-oEUnt4tk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(200,200)) #???\n",
        "nx.draw_networkx(G, pos = nx.drawing.layout.bipartite_layout(G, [word for word in unique_words]), width = 2, font_size=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWywZQMg2qHK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTuiTAIIgLra"
      },
      "source": [
        "**Punkt 3** : Analiza bigramów\n",
        "\n",
        "Badać bigramy w ramach akapitów.\n",
        "```\n",
        "Wyr1 wyr2 wyr3 wyr4 wyr5 wyr6 ….\n",
        "\n",
        "Pary sąsiadujących ze sobą wyrazów -2 sąsiednie.\n",
        "\n",
        "Wyr1   wyr2      [ilosc wystąpień]\n",
        "Wyr2   wyr3      [ilosc wystąpień]\n",
        "Wyr3   wyr4      [ilosc wystąpień]\n",
        "Wyr4   wyr5      [ilosc wystąpień]\n",
        "```\n",
        "W manuskrypie większość bigramów będą pojawiające się 1 raz.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zle_akapity = 0\n",
        "akapit_poprawny = 0\n",
        "structure1 = [[]]\n",
        "\n",
        "with open(\"wynik.txt\", \"r\") as wynik:\n",
        "  for line in wynik:\n",
        "    words = line.split(',')\n",
        "    if len(words) == 1  and ('=' in words[0] or '#'in words[0]): zle_akapity+=1\n",
        "    else: \n",
        "      if('=' in words[-1]):\n",
        "        words[-1] = words[-1].replace('=','')\n",
        "        structure1[akapit_poprawny] += words\n",
        "        akapit_poprawny+=1\n",
        "        structure1.append([])\n",
        "      else: structure1[akapit_poprawny] += words\n",
        "\n",
        "# wszystkie pary słow wg jednego akapitu  -  Nie ma par typu [ostatnie słowo z akapitu, pierwsze słowo z akapitu nastepnego]\n",
        "pary_slow = []\n",
        "\n",
        "for i in structure1 :\n",
        "  for j in range(len(i)-1) :\n",
        "    if( '-'  in i[j] ): i[j]= i[j].replace('-','')\n",
        "    if( '-'  in i[j+1] ): i[j+1] = i[j+1].replace('-','')\n",
        "    if( '\\n'  in i[j] ): i[j] = i[j].replace('\\n','')\n",
        "    if( '\\n'  in i[j+1] ): i[j+1] = i[j+1].replace('\\n','')\n",
        "    pary_slow.append([i[j], i[j+1]])\n",
        "\n",
        "pary_slow\n",
        "print(\"Ilość par słow wg jednego akapitu: \", len(pary_slow))\n",
        "\n",
        "# unikalna lista par\n",
        "unique_pary_slow = []\n",
        "for i in pary_slow:\n",
        "    if i not in unique_pary_slow:\n",
        "        unique_pary_slow.append(i)\n",
        "print(\"Ilość unikalnych par słow: \" , len(unique_pary_slow))\n",
        "\n",
        "\n",
        "counter = 0\n",
        "for i in unique_pary_slow:\n",
        "  for j in pary_slow:\n",
        "    if(i == j): counter += 1\n",
        "  i.append(counter)\n",
        "  counter = 0\n",
        "\n",
        "# posortowanie po ilosci wystapien\n",
        "unique_pary_slow.sort(key = lambda i: -i[2])\n",
        "\n",
        "# wyswietlenie wyniku\n",
        "for i in unique_pary_slow:\n",
        "  print(i[0],\"   \", i[1], \"  ilośc wystapień: \", i[2])\n"
      ],
      "metadata": {
        "id": "S7ERV65N9qfa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('WPTG')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "42fe00b64b01f2bab451862d995f64a734eb7708d589cd05273de4d395b3959d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}